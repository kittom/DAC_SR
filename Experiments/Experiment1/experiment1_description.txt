Experiment 1: Learning Ground Truth Equations for Benchmark Problems

Objective:
Evaluate how well different models can learn the ground truth equations for three benchmark problems: LeadingOnes, OneMax, and PSA-CMA-ES benchmarks (Sphere, Ellipsoid, Rastrigin, Noisy Ellipsoid, Schaffer, Noisy Rastrigin).

Setup:
- For LeadingOnes and OneMax:
  - Generate datasets with portfolio sizes: 10, 20, 30, 40, 50, 100, 200, 500.
  - Each dataset will be used to train and evaluate the model's ability to learn the ground truth equation.
- For PSA-CMA-ES:
  - Run 1000 iterations of the model for each benchmark (Sphere, Ellipsoid, Rastrigin, Noisy Ellipsoid, Schaffer, Noisy Rastrigin).
  - Output a separate CSV file for each benchmark, containing the relevant variables and results.

Data Storage:
- All generated datasets and results for this experiment will be stored in the Datasets directory within this subdirectory.
- Each problem type (OneMax, LeadingOnes, PSA-CMA-ES) has two subdirectories:
  - continuous/: Contains continuous data for control and tailored library evaluations
  - discrete/: Contains discrete data (with rounded values) for rounding evaluations

Evaluation:
- Model performance will be evaluated using modular scripts that call the run_all scripts in the Scripts directory.
- Three evaluation approaches are available:
  1. Control Library: Using run_all_sr.sh with complete mathematical function libraries (on continuous data)
  2. Tailored Library: Using run_all_library.sh with problem-specific minimal function libraries (on continuous data)
  3. Rounding: Using run_all_sr_rounding.sh with rounding-enabled evaluation (on discrete data)
           - Results will be saved to:
           - results.csv: Control library results (full mathematical function set) - from continuous data
           - results_lib.csv: Tailored library results (minimal function set) - from continuous data
           - results_rounding.csv: Rounding-enabled results - from discrete data
           - Each results file contains the ground truth equation as the first column for easy analysis
- This allows direct comparison between control, tailored library, and rounding performance on appropriate data types.

Scripts:
- generate_data.sh: Modular data generation with options for individual datasets
- run_evaluation.sh: Modular evaluation with options for evaluation types and datasets
- run_all_evaluations.sh: Master script for running all evaluation types
- script_utils/: Contains reusable generation and evaluation components

FAIRNESS CONSIDERATIONS FOR EXPERIMENT 1:
=========================================

1. Stopping Criteria Implementation:
   - DeepSR: Uses threshold-based stopping with noise level as convergence criterion
   - Linear Regression: Uses sklearn's built-in convergence
   - PySR: Modified to use convergence-based stopping (max 100 iterations, min 10 iterations)
   - KAN: Modified to use convergence-based stopping (max 100 steps, min 20 steps)
   - TPSR: Modified to use convergence-based stopping (max 200 horizon, min 10 horizon)
   - Q-Lattice: Limited to time/complexity-based stopping (no loss-based control)
   - E2E Transformer: No stopping criteria (inference only)

2. Convergence Threshold:
   - All algorithms use noise_level as the convergence threshold
   - For noiseless data: threshold = 1e-12 (machine precision)
   - For noisy data: threshold = noise_level
   - Ensures fair comparison based on problem difficulty

3. Algorithm-Specific Fairness Measures:
   - Each algorithm uses its native loss function (no forced standardization)
   - Computational budgets are balanced through iteration/time limits
   - All algorithms receive the same datasets and evaluation metrics
   - Linear regression serves as baseline control

4. Evaluation Standardization:
   - All final comparisons use NMSE (Normalized Mean Square Error)
   - Ground truth equations use standardized x1, x2, ..., xn notation
   - Results stored in consistent CSV format across all algorithms

5. Limitations and Caveats:
   - Q-Lattice and E2E Transformer have limited stopping criteria control
   - Different algorithms may have different convergence behaviors
   - Some algorithms may not converge within the specified limits
   - All limitations are documented in results and analysis

This experiment follows the fairness standards outlined in the root standards.md document. 