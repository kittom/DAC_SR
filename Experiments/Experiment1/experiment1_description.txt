Experiment 1: Learning Ground Truth Equations for Benchmark Problems

Objective:
Evaluate how well different models can learn the ground truth equations for three benchmark problems: LeadingOnes, OneMax, and PSA-CMA-ES benchmarks (Sphere, Ellipsoid, Rastrigin, Noisy Ellipsoid, Schaffer, Noisy Rastrigin).

Setup:
- For LeadingOnes and OneMax:
  - Generate datasets with portfolio sizes: 10, 20, 30, 40, 50, 100, 200, 500.
  - Each dataset will be used to train and evaluate the model's ability to learn the ground truth equation.
- For PSA-CMA-ES:
  - Run 1000 iterations of the model for each benchmark (Sphere, Ellipsoid, Rastrigin, Noisy Ellipsoid, Schaffer, Noisy Rastrigin).
  - Output a separate CSV file for each benchmark, containing the relevant variables and results.

Data Storage:
- All generated datasets and results for this experiment will be stored in the Datasets directory within this subdirectory.

Evaluation:
- Model performance will be evaluated using the run_all scripts in the Scripts directory, comparing learned equations to ground truth.
- Three evaluation approaches will be run simultaneously:
  1. Control Library: Using run_all_sr.sh with complete mathematical function libraries
  2. Tailored Library: Using run_all_library.sh with problem-specific minimal function libraries
  3. Rounding: Using run_all_rounding.sh with rounding-enabled evaluation
- Results will be saved to:
  - results.csv: Control library results (full mathematical function set)
  - results_lib.csv: Tailored library results (minimal function set)
  - results_rounding.csv: Rounding-enabled results
- This allows direct comparison between control, tailored library, and rounding performance on identical datasets.

FAIRNESS CONSIDERATIONS FOR EXPERIMENT 1:
=========================================

1. Stopping Criteria Implementation:
   - DeepSR: Uses threshold-based stopping with noise level as convergence criterion
   - Linear Regression: Uses sklearn's built-in convergence
   - PySR: Modified to use convergence-based stopping (max 100 iterations, min 10 iterations)
   - KAN: Modified to use convergence-based stopping (max 100 steps, min 20 steps)
   - TPSR: Modified to use convergence-based stopping (max 200 horizon, min 10 horizon)
   - Q-Lattice: Limited to time/complexity-based stopping (no loss-based control)
   - E2E Transformer: No stopping criteria (inference only)

2. Convergence Threshold:
   - All algorithms use noise_level as the convergence threshold
   - For noiseless data: threshold = 1e-12 (machine precision)
   - For noisy data: threshold = noise_level
   - Ensures fair comparison based on problem difficulty

3. Algorithm-Specific Fairness Measures:
   - Each algorithm uses its native loss function (no forced standardization)
   - Computational budgets are balanced through iteration/time limits
   - All algorithms receive the same datasets and evaluation metrics
   - Linear regression serves as baseline control

4. Evaluation Standardization:
   - All final comparisons use NMSE (Normalized Mean Square Error)
   - Ground truth equations use standardized x1, x2, ..., xn notation
   - Results stored in consistent CSV format across all algorithms

5. Limitations and Caveats:
   - Q-Lattice and E2E Transformer have limited stopping criteria control
   - Different algorithms may have different convergence behaviors
   - Some algorithms may not converge within the specified limits
   - All limitations are documented in results and analysis

This experiment follows the fairness standards outlined in the root standards.md document. 